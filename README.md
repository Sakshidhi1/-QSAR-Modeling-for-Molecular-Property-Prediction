# ğŸ§ª QSAR-Modeling-for-Molecular-Property-Prediction

This project involves building Quantitative Structureâ€“Activity Relationship (QSAR) models to predict molecular properties based on chemical descriptors and fingerprints. The models were trained and evaluated using various techniques including Ridge Regression and XGBoost.

## ğŸ“‚ Dataset

- Molecular descriptors and fingerprints
- Total samples: 12150
- Features: 7541 after combination of fingerprints and scaled descriptors

 ## âš™ï¸ Preprocessing

- *Scaling*: StandardScaler applied to descriptors
- *Combining*: Descriptors and fingerprints stacked horizontally
- *Dimensionality Reduction*: PCA used for Ridge Regression model
- *Train/Test Split*: 80/20

## ğŸ§  Models Compared

ğŸ”¹ Ridge Regression (Linear)

Used after PCA to handle high-dimensional features

âœ… Cross-Validation RÂ²: 0.50 Â± 0.01

âœ… Test RÂ²: 0.4855

âœ… Test RMSE: 0.911

ğŸ”¹ XGBoost Regressor (Non-Linear)

Non-linear gradient boosting model

âš ï¸ Cross-Validation RÂ²: 0.43 Â± 0.19 (higher variance)

âœ… Test RÂ²: 0.68

âœ… Test RMSE: 0.56

ğŸ“Š Model Comparison Summary

## Model	CV RÂ² Score	Test RÂ²	Test RMSE

Ridge + PCA	0.50 Â± 0.01	0.4855	0.911
XGBoost	0.43 Â± 0.19	0.68	0.56


> ğŸ” Insight: While XGBoost performed better on the test set, it showed high variance in cross-validation. Ridge Regression was more stable, making it a safer generalization choice.


## ğŸ“ˆ Visualizations

ğŸ“Œ Residual Distribution (Ridge & XGBoost)

ğŸ“Œ Predicted vs. Actual Plot

ğŸ“Œ Feature Importance (XGBoost only)

## ğŸ—‚ï¸ File Structure

ğŸ“ qsar-model/
â”œâ”€â”€ egfr.csv
â”œâ”€â”€ molecular_descriptors.csv
â”œâ”€â”€ ridge_model.pkl
â”œâ”€â”€ pca_transform.pkl
â”œâ”€â”€ ridge_qsar.png
â”œâ”€â”€ xgboost_qsar.png
â”œâ”€â”€ QSAR_ML.ipynb
â””â”€â”€ README.md


## ğŸ“¦ Requirements

pip install rdkit scikit-learn pandas xgboost matplotlib seaborn


## âœ… Conclusion

Only Ridge Regression and XGBoost were compared.

Ridge provided stable and explainable results despite lower test RÂ².

XGBoost, while more accurate on the test set, showed high variability and possible overfitting.

Future work can explore:

Ensembling both models

Feature selection

Additional regression methods like SVR or Random Forest
